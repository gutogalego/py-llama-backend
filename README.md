# py-llama-backend


Requirements:

Download ollama https://ollama.com/download

run ollama locally with `ollama serve`

we are using the `wizardlm2:7b` model. We don't need it to be the most accurate one, just the fastest for our intents. You can use any model you like.